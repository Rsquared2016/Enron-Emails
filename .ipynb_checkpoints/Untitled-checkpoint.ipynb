{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named feature_format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ebcb2c98c442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tools/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mdata_tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Nan\\Documents\\Data Analysis\\DAND_Enron_Emails\\data_tools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tools/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named feature_format"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from copy import copy\n",
    "sys.path.append(\"/tools/\")\n",
    "\n",
    "import data_tools\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "financial_features = ['salary', \n",
    "                      'deferral_payments', \n",
    "                      'total_payments', \n",
    "                      'loan_advances', \n",
    "                      'bonus', \n",
    "                      'restricted_stock_deferred', \n",
    "                      'deferred_income', \n",
    "                      'total_stock_value', \n",
    "                      'expenses', \n",
    "                      'exercised_stock_options', \n",
    "                      'other', \n",
    "                      'long_term_incentive', \n",
    "                      'restricted_stock', \n",
    "                      'director_fees'] \n",
    "\n",
    "email_features= ['to_messages', \n",
    "                #'email_address',  # remit email address label\n",
    "                 'from_poi_to_this_person', \n",
    "                 'from_messages', \n",
    "                 'from_this_person_to_poi', \n",
    "                 'shared_receipt_with_poi'] \n",
    "\n",
    "POI_label= ['poi'] \n",
    "\n",
    "feature_list = POI_label +  financial_features + email_features # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "### Explore dataset \n",
    "total_data_point, valid_data_point, number_of_poi, number_of_features, \\\n",
    "    features_list, valid_data_by_feature, invalid_data_by_person = data_tools.explore_data(data_dict)\n",
    "print \"Total data point in enron dataset:\", total_data_point, \"\\n\"\n",
    "print \"Valid data point in enron dataset:\", valid_data_point, \"\\n\"\n",
    "print \"Number of poi in enron dataset:\", number_of_poi, \"\\n\"\n",
    "print \"Number of features in enron dataset:\", number_of_features, \"\\n\"     \n",
    "\n",
    "### Task 2: Remove outliers\n",
    "outliers = ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E']\n",
    "for outlier in outliers:\n",
    "    data_dict.pop(outlier, 0)\n",
    "        \n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = copy(data_dict)\n",
    "my_feature_list = copy(feature_list)\n",
    " \n",
    "### Task 3: Create new feature(s)\n",
    "data_tools.add_feature_financial_sum(my_dataset, my_feature_list)\n",
    "data_tools.add_feature_message_proportion_with_poi(my_dataset, my_feature_list)\n",
    "\n",
    "### Get K-best features \n",
    "k_features = 10\n",
    "k_best_features = data_tools.get_k_best(my_dataset, my_feature_list, k_features)\n",
    "my_feature_list = POI_label + k_best_features.keys()\n",
    "\n",
    "### Print selected feature names and score\n",
    "print \"{0} selected features: {1}\\n\".format(len(my_feature_list) - 1, my_feature_list[1:])\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Rescale features via StandardScaler transformation\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "### Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "### Support Vector Machine (SVM) Classifier\n",
    "from sklearn.svm import SVC\n",
    "s_clf = SVC(kernel='rbf', C=1000, class_weight='balanced')\n",
    "\n",
    "### Nearest Neighbors Classifer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf =  KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "### Stochastic Gradient Descent - Logistic Regression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "g_clf = SGDClassifier(loss='log', class_weight = \"balanced\")\n",
    "\n",
    "### K-means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "k_clf = KMeans(n_clusters=2, tol=0.001)\n",
    "\n",
    "### Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "### parameter optimization \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### tune parameters for K-means Clustering; uncomment to run\n",
    "########################################\n",
    "### trails = 1\n",
    "### parameters = {\"tol\": [1e-15, 1e-10, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]}\n",
    "### k_clf = GridSearchCV(k_clf, parameters) \n",
    "### print \"(score, precision, recall) using optimized Stochastic Gradient Descent:\",\\\n",
    "###    data_tools.evaluate_clf(k_clf, features, labels, trials, test_size, is_pca, n_components), \"\\n\"\n",
    "### k_clf = k_clf.best_estimator_\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "### Algorithm evaluations\n",
    "is_pca = False  \n",
    "trials = 1000\n",
    "test_size = 0.3\n",
    "n_components = 5\n",
    "\n",
    "print \"(score, precision, recall) using GaussianNB:\",\\\n",
    "    data_tools.evaluate_clf(nb_clf, features, labels, trials, test_size,  is_pca, n_components), \"\\n\"\n",
    "print \"(score, precision, recall) using Support Vector Machines:\",\\\n",
    "    data_tools.evaluate_clf(s_clf, features, labels, trials, test_size, is_pca, n_components), \"\\n\"\n",
    "print \"(score, precision, recall) using Nearest Neighbors:\",\\\n",
    "    data_tools.evaluate_clf(nb_clf, features, labels, trials, test_size,  is_pca, n_components), \"\\n\"\n",
    "print \"(score, precision, recall) using Stochastic Gradient Descent:\",\\\n",
    "    data_tools.evaluate_clf(g_clf, features, labels, trials, test_size, is_pca, n_components), \"\\n\"\n",
    "print \"(score, precision, recall) using K-means:\",\\\n",
    "    data_tools.evaluate_clf(k_clf, features, labels, trials, test_size, is_pca, n_components), \"\\n\"\n",
    "print \"(score, precision, recall) using Random Forests:\",\\\n",
    "    data_tools.evaluate_clf(rf_clf, features, labels, trials, test_size, is_pca, n_components), \"\\n\"\n",
    "\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = nb_clf\n",
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "pickle.dump(clf, open(\"my_classifier.pkl\", \"w\"))\n",
    "pickle.dump(my_dataset, open(\"my_dataset.pkl\", \"w\"))\n",
    "pickle.dump(my_feature_list, open(\"my_feature_list.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
